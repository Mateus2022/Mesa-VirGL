.test:
  # Cancel job if a newer commit is pushed to the same branch
  interruptible: true
  variables:
    GIT_STRATEGY: none # testing doesn't build anything from source
  before_script:
    - !reference [default, before_script]
    # Note: Build dir (and thus install) may be dirty due to GIT_STRATEGY
    - rm -rf install
    - tar -xf artifacts/install.tar
    - section_start ldd_section "Checking ldd on driver build"
    - LD_LIBRARY_PATH=install/lib find install/lib -name "*.so" -print -exec ldd {} \;
    - section_end ldd_section
  artifacts:
    when: always
    name: "mesa_${CI_JOB_NAME}"
    paths:
      - results/

.formatting-check:
  # Cancel job if a newer commit is pushed to the same branch
  interruptible: true
  stage: lint
  extends:
    - .use-debian/x86_64_build
  rules:
    # in merge pipeline, don't touch the default settings
    - if: $GITLAB_USER_LOGIN == "marge-bot" && $CI_COMMIT_BRANCH == null
    # in other pipelines, formatting checks are allowed to fail
    - allow_failure: true
  variables:
    GIT_STRATEGY: fetch
  timeout: 10m
  script:
    - git diff --color=always --exit-code  # Fails if there are diffs
  tags:
    - placeholder-job

rustfmt:
  extends:
    - .formatting-check
    - .lint-rustfmt-rules
  before_script:
    - shopt -s globstar
    - rustfmt --version
    - rustfmt --verbose src/**/lib.rs

clang-format:
  extends:
    - .formatting-check
    - .lint-clang-format-rules
  variables:
    LLVM_VERSION: 15
  before_script:
    - shopt -s globstar
    # We need a meson build dir, but its config doesn't actually matter, so
    # let's just use the default.
    - meson setup build
    - clang-format-${LLVM_VERSION} --version
    - ninja -C build clang-format

.test-gl:
  extends:
    - .test
    - .use-debian/x86_64_test-gl
  needs:
    - debian/x86_64_test-gl
    - debian-testing
    - !reference [.required-for-hardware-jobs, needs]
  variables:
    DEBIAN_ARCH: amd64

.test-vk:
  extends:
    - .test
    - .use-debian/x86_64_test-vk
  needs:
    - debian-testing
    - debian/x86_64_test-vk
    - !reference [.required-for-hardware-jobs, needs]
  variables:
    DEBIAN_ARCH: amd64

.test-cl:
  extends:
    - .test
    - .use-debian/x86_64_test-gl
  needs:
    - debian/x86_64_test-gl
    - !reference [.required-for-hardware-jobs, needs]

.test-android:
  extends:
    - .test
    - .use-debian/x86_64_test-android
  variables:
    S3_ARTIFACT_NAME: mesa-x86_64-android-debug
  needs:
    - job: debian-testing
      artifacts: true  # On the host we want the Linux build
    - job: debian-android
      artifacts: false  # The Android build will be downloaded later
    - job: debian/x86_64_test-android
      artifacts: false
    - !reference [.required-for-hardware-jobs, needs]
  timeout: 20m
  script:
    - ./install/cuttlefish-runner.sh
  artifacts:
    paths:
      - results/

.vkd3d-proton-test:
  artifacts:
    when: on_failure
    name: "mesa_${CI_JOB_NAME}"
    paths:
      - results/vkd3d-proton.log
  script:
    - ./install/vkd3d-proton/run.sh

.piglit-test:
  artifacts:
    name: "mesa_${CI_JOB_NAME}"
    paths:
      - results
    reports:
      junit: results/junit.xml
  variables:
    PIGLIT_NO_WINDOW: 1
    HWCI_TEST_SCRIPT: "/install/piglit/piglit-runner.sh"
  script:
    - install/piglit/piglit-runner.sh

.piglit-traces-test:
  extends:
    - .piglit-test
  artifacts:
    when: on_failure
    name: "mesa_${CI_JOB_NAME}"
    reports:
      junit: results/junit.xml
    paths:
      - results/
    exclude:
      - results/*.shader_cache
  variables:
    PIGLIT_REPLAY_EXTRA_ARGS: --db-path ${CI_PROJECT_DIR}/replayer-db/ --minio_bucket=mesa-tracie-public --jwt-file=${CI_JOB_JWT_FILE}
    # until we overcome Infrastructure issues, give traces extra 5 min before timeout
    DEVICE_HANGING_TIMEOUT_SEC: 600
  script:
    - section_start variables "Variables passed through:"
    - install/common/generate-env.sh
    - section_end variables
    - install/piglit/piglit-traces.sh

.deqp-test:
  script:
    - rm -rf results # Clear out old results if the docker container was cached
    - ./install/deqp-runner.sh
  artifacts:
    exclude:
      - results/*.shader_cache
    reports:
      junit: results/junit.xml

.deqp-test-vk:
  extends:
    - .deqp-test
  variables:
    DEQP_VER: vk

.fossilize-test:
  script:
    - ./install/fossilize-runner.sh
  artifacts:
    when: on_failure
    name: "mesa_${CI_JOB_NAME}"
    paths:
      - results/

.baremetal-test:
  extends:
    - .test
  # Cancel job if a newer commit is pushed to the same branch
  interruptible: true
  before_script:
    - !reference [default, before_script]
    # Use this instead of gitlab's artifacts download because it hits packet.net
    # instead of fd.o.  Set FDO_HTTP_CACHE_URI to an http cache for your test lab to
    # improve it even more (see https://docs.mesa3d.org/ci/bare-metal.html for
    # setup).
    - section_start artifacts_download "Downloading artifacts from s3"
    # Note: Build dir (and thus install) may be dirty due to GIT_STRATEGY
    - rm -rf install
    - (set -x; curl -L --retry 4 -f --retry-all-errors --retry-delay 60 ${FDO_HTTP_CACHE_URI:-}https://${PIPELINE_ARTIFACTS_BASE}/${S3_ARTIFACT_NAME}.tar.zst | tar --zstd -x)
    - section_end artifacts_download
  variables:
    BM_ROOTFS: /rootfs-${DEBIAN_ARCH}
  artifacts:
    when: always
    name: "mesa_${CI_JOB_NAME}"
    paths:
      - results/
      - serial*.txt
    exclude:
      - results/*.shader_cache
    reports:
      junit: results/junit.xml

# ARM testing of bare-metal boards attached to an x86 gitlab-runner system
.baremetal-test-arm32:
  extends:
    - .baremetal-test
    - .use-debian/arm32_test
  variables:
    DEBIAN_ARCH: armhf
    S3_ARTIFACT_NAME: mesa-arm32-default-debugoptimized
  needs:
    - debian/arm32_test
    - job: debian-arm32
      artifacts: false
    - !reference [.required-for-hardware-jobs, needs]

# ARM64 testing of bare-metal boards attached to an x86 gitlab-runner system
.baremetal-test-arm64:
  extends:
    - .baremetal-test
    - .use-debian/arm64_test
  variables:
    DEBIAN_ARCH: arm64
    S3_ARTIFACT_NAME: mesa-arm64-default-debugoptimized
  needs:
    - debian/arm64_test
    - job: debian-arm64
      artifacts: false
    - !reference [.required-for-hardware-jobs, needs]

# ARM32/64 testing of bare-metal boards attached to an x86 gitlab-runner system, using an asan mesa build
.baremetal-arm32-asan-test:
  extends:
    - .baremetal-test
    - .use-debian/arm32_test
  variables:
    DEQP_RUNNER_OPTIONS: "--env LD_PRELOAD=libasan.so.8:/install/lib/libdlclose-skip.so"
    S3_ARTIFACT_NAME: mesa-arm32-asan-debugoptimized
  needs:
    - debian/arm32_test
    - job: debian-arm32-asan
      artifacts: false
    - !reference [.required-for-hardware-jobs, needs]

.baremetal-arm64-asan-test:
  extends:
    - .baremetal-test
    - .use-debian/arm64_test
  variables:
    DEQP_RUNNER_OPTIONS: "--env LD_PRELOAD=libasan.so.8:/install/lib/libdlclose-skip.so"
    S3_ARTIFACT_NAME: mesa-arm64-asan-debugoptimized
  needs:
    - debian/arm64_test
    - job: debian-arm64-asan
      artifacts: false
    - !reference [.required-for-hardware-jobs, needs]

.baremetal-deqp-test:
  variables:
    HWCI_TEST_SCRIPT: "/install/deqp-runner.sh"
    FDO_CI_CONCURRENT: 0 # Default to number of CPUs

# For Valve's bare-metal testing farm jobs.
.b2c-test:
  # It would be nice to use ci-templates within Mesa CI for this job's
  # image:, but the integration is not possible for the current
  # use-case. Within this job, two containers are managed. 1) the
  # gitlab runner container from which the job is submitted to the
  # DUT, and 2) the test container (e.g. debian/x86_64_test-vk) within
  # which the test cases will run on the DUT. Since ci-templates and
  # the associated image setting macros in this file rely on variables
  # like FDO_DISTRIBUTION_TAG for *the* image, there is no way to
      # The Valve CI gateway receives jobs in a YAML format. Create a
      # job description from the CI environment.
      python3 "$CI_B2C_ARTIFACTS"/generate_b2c.py \
        --ci-job-id "${CI_JOB_ID}" \
        --container-cmd "${B2C_TEST_SCRIPT}" \
        --initramfs-url "${B2C_INITRAMFS_URL}" \
        --job-success-regex "${B2C_JOB_SUCCESS_REGEX}" \
        --job-warn-regex "${B2C_JOB_WARN_REGEX}" \
        --kernel-url "${B2C_KERNEL_URL}" \
        --log-level "${B2C_LOG_LEVEL}" \
        --poweroff-delay "${B2C_POWEROFF_DELAY}" \
        --session-end-regex "${B2C_SESSION_END_REGEX}" \
        --session-reboot-regex "${B2C_SESSION_REBOOT_REGEX}" \
        --tags "${CI_RUNNER_TAGS}" \
        --template "${B2C_JOB_TEMPLATE}" \
        --timeout-boot-minutes "${B2C_TIMEOUT_BOOT_MINUTES}" \
        --timeout-boot-retries "${B2C_TIMEOUT_BOOT_RETRIES}" \
        --timeout-first-minutes "${B2C_TIMEOUT_FIRST_MINUTES}" \
        --timeout-first-retries "${B2C_TIMEOUT_FIRST_RETRIES}" \
        --timeout-minutes "${B2C_TIMEOUT_MINUTES}" \
        --timeout-overall-minutes "${B2C_TIMEOUT_OVERALL_MINUTES}" \
        --timeout-retries "${B2C_TIMEOUT_RETRIES}" \
        --job-volume-exclusions "${B2C_JOB_VOLUME_EXCLUSIONS}" \
        --local-container "${IMAGE_UNDER_TEST}" \
        ${B2C_EXTRA_VOLUME_ARGS} \
        --working-dir "$CI_PROJECT_DIR"

      cat b2c.yml.jinja2

      rm -rf ${JOB_FOLDER} || true
      mkdir -v ${JOB_FOLDER}

      # Create a script to regenerate the CI environment when this job
      # begins running on the remote DUT.
      set +x
      "$CI_COMMON_SCRIPTS"/generate-env.sh > ${JOB_FOLDER}/set-job-env-vars.sh
      echo "export SCRIPTS_DIR=./install" >> ${JOB_FOLDER}/set-job-env-vars.sh
      echo "Variables passed through:"
      cat ${JOB_FOLDER}/set-job-env-vars.sh
      echo "export CI_JOB_JWT=${CI_JOB_JWT}" >> ${JOB_FOLDER}/set-job-env-vars.sh
      set -x

      # Copy the mesa install tarball to the job folder, for later extraction
      mv "${INSTALL_TARBALL}" "${JOB_FOLDER}"

  script: |
      slugify () {
          echo "$1" | sed -r s/[~\^]+//g | sed -r s/[^a-zA-Z0-9]+/-/g | sed -r s/^-+\|-+$//g | tr A-Z a-z
      }

      # Submit the job to Valve's CI gateway service with the CI
      # provisioned job_folder.
      env PYTHONUNBUFFERED=1 executorctl \
          run -w b2c.yml.jinja2 -j $(slugify "$CI_JOB_NAME") -s ${JOB_FOLDER} -i "$CI_RUNNER_DESCRIPTION"

      # Anything our job places in results/ will be collected by the
      # Gitlab coordinator for status presentation. results/junit.xml
      # will be parsed by the UI for more detailed explanations of
      # test execution.

  after_script:
      # Keep the results path the same as baremetal and LAVA
      - mkdir -p "${JOB_FOLDER}"/results
      - mv "${JOB_FOLDER}"/results results/
      - !reference [default, after_script]

  artifacts:
    when: always
    name: "mesa_${CI_JOB_NAME}"
    paths:
      - results
    reports:
      junit: results/**/junit.xml

.b2c-test-vk:
  extends:
    - .use-debian/x86_64_test-vk
    - .b2c-test
  needs:
    - debian/x86_64_test-vk
    - debian-testing
    - !reference [.required-for-hardware-jobs, needs]

.b2c-test-gl:
  extends:
    - .use-debian/x86_64_test-gl
    - .b2c-test
  needs:
    - debian/x86_64_test-gl
    - debian-testing
    - !reference [.required-for-hardware-jobs, needs]


